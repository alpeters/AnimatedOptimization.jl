{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "---\ntitle       : \"Optimization\"\nsubtitle    : \nauthor      : Paul Schrimpf\ndate        : `j using Dates; print(Dates.today())`\nbibliography: \"opt.bib\"\n---\n\n[![](https://i.creativecommons.org/l/by-sa/4.0/88x31.png)](http://creativecommons.org/licenses/by-sa/4.0/)\n\nThis work is licensed under a [Creative Commons Attribution-ShareAlike\n4.0 International\nLicense](http://creativecommons.org/licenses/by-sa/4.0/) \n\n### About this document {-}\n\nThis document was created using Weave.jl. The code is available \n[on github](https://github.com/schrimpf/AnimatedOptimization.jl/). The same\ndocument generates both the [static webpage](https://schrimpf.github.io/AnimatedOptimization.jl/optimization/) \nand associated [jupyter notebook](https://schrimpf.github.io/AnimatedOptimization.jl/optimization.ipynb) ([or on nbviewer](https://nbviewer.jupyter.org/urls/schrimpf.github.io/AnimatedOptimization.jl/optimization.ipynb)).\n\n$$\n\\def\\indep{\\perp\\!\\!\\!\\perp}\n\\def\\Er{\\mathrm{E}}\n\\def\\R{\\mathbb{R}}\n\\def\\En{{\\mathbb{E}_n}}\n\\def\\Pr{\\mathrm{P}}\n\\newcommand{\\norm}[1]{\\left\\Vert {#1} \\right\\Vert}\n\\newcommand{\\abs}[1]{\\left\\vert {#1} \\right\\vert}\n\\DeclareMathOperator*{\\argmax}{arg\\,max}\n\\DeclareMathOperator*{\\argmin}{arg\\,min}\n\\def\\inprob{\\,{\\buildrel p \\over \\rightarrow}\\,} \n\\def\\indist{\\,{\\buildrel d \\over \\rightarrow}\\,} \n$$\n\n# Optimization Algorithms\n\nThe goal of this notebook is to give you some familiarity with numeric\noptimization. \n\nNumeric optimization is important because many (most) models cannot be\nfully solved analytically. Numeric results can be used to complement\nanalytic ones. Numeric optimization plays a huge role in econometrics. \n\nIn these notes, we will focus on minimization problems following the\nconvention in mathematics, engineering, and most numerical\nlibraries. It is easy to convert between minimization and\nmaximization, and we hope that this does not lead to any confusion.\n\n# Heuristic searches\n\nThe simplest type of optimization algorithm are heuristic\nsearches. Consider the problem: \n\n$$\n\\min_x f(x)\n$$\n\nwith $f:\\R^n \\to \\R$. Heuristic search algorithms consist of \n\n1. Evaluate $f$ at a collection of points \n2. Generate a new candidate point, $x^{new}$. Replace a point\n   in the current collection with $x^{new}$ if $f(x^{new})$ is small enough. \n3. Stop when function value stops decreasing and/or collection of\n   points become too close together. \n   \nThere are many variants of such algorithms with different ways of\ngenerating new points, deciding whether to accept the new point, and\ndeciding when to stop.  Here is a simple implementation and animation\nof the above idea. In the code below, new points are drawn randomly\nfrom a normal distribution, and new points are accepted whenever\n$f(x^{new})$ is smaller than the worst existing function value."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "using Distributions, Plots\nmarkdown = false\ntry\n  markdown = \"md\" in keys(WEAVE_ARGS) && WEAVE_ARGS[\"md\"]\ncatch\n  markdown = false\nend\nif markdown\n  ENV[\"GKSwstype\"]=\"nul\" # for running Plots with GR backend without DISPLAY (e.g. over ssh)\nend"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "try \n  using AnimatedOptimization\ncatch\n  using Pkg\n  Pkg.add(Pkg.Spec(url=\"https://github.com/schrimpf/AnimatedOptimization.jl\"))\n  using AnimatedOptimization\nend"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "function functiontext(functionname, filename; includedoc=true)\n  lines = readlines(filename)\n  fstart=findfirst(occursin.(Regex(\"function\\\\s+$(functionname)\"),lines))\n  fend  =fstart + findfirst(occursin.(r\"^end\",lines[(fstart+1):end]))  \n  if (includedoc && occursin(r\"^\\\"\\\"\\\"\",lines[fstart-1]) )\n    dend = fstart -1\n    dstart = dend - findfirst(occursin.(r\"^\\\"\\\"\\\"\", lines[(fstart-2):(-1):1]))\n  end\n  lines[dstart:fend]\nend\n\nfunction printfunc(functionname, srcfile; pkg=AnimatedOptimization)\n  filename=joinpath(dirname(Base.pathof(pkg)),srcfile)\n  println.(functiontext(functionname,filename))\n  return nothing\nend"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "markdown && printfunc(\"minrandomsearch\",\"heuristic_optimizers.jl\");"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "\"\"\"\n     banana(a,b)\n  \n  Returns the Rosenbrock function with parameters a, b.\n\"\"\"\nfunction banana(a,b)\n  x->(a-x[1])^2+b*(x[2]-x[1]^2)^2\nend\nf = banana(1.0,1.0)\n\nx0 = [-2.0, 3.0];"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "result = minrandomsearch(f, x0, 20, var0=0.1, vshrink=0.5, vtol=1e-3 )\ngif(result[5], \"randsearch.gif\", fps=5)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "![random search](randsearch.gif)\n\nThere are many other heuristic search algorithms. A popular\ndeterministic one is the Nelder-Mead simplex. Popular heuristic\nsearch algorithms that include some randomness include simulated\nannealing and particle swarm. Each of the three algorithms just\nmentioned are available in\n[Optim.jl](https://julianlsolvers.github.io/Optim.jl/stable/#algo/nelder_mead/). These\nheuristic searches have the advantage that they only function values\n(as opposed to also requiring gradients or hessians, see\nbelow). Some heuristic algorithms, like simulated annealing, can be\nshown to converge to a global (instead of local) minimum under\nappropriate assumptions. Compared to algorithms that use more\ninformation, heuristic algorithms tend to require many more function\nevaluations. \n\n# Gradient descent\n\nGradient descent is an iterative algorithm to find a local minimum. As\nthe name suggests, it consists of descending toward a minimum in the\ndirection opposite the gradient. Each step, you start at some $x$ and\ncompute $x_{new}$\n\n1. Given current $x$, compute $x_{new} = x - \\gamma Df_{x}$\n2. Adjust $\\gamma$ depending on whether $f(x_{new})<f(x)$\n3. Repeat until $\\norm{Df_{x}}$, $\\norm{x-x_{new}}$, and/or\n   $\\abs{f(x)-f(x_{new})}$ small."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "using ForwardDiff, LinearAlgebra"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "markdown && printfunc(\"graddescent\",\"smooth_optimizers.jl\");"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "result = graddescent(f, x0)\ngif(result[5], \"graddescent.gif\", fps=10);"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "![gradient descent](graddescent.gif)\n\n\nAlthough an appealing and intuitive idea, the above example\nillustrates that gradient descent can perform surprisingly poorly in\nsome cases. Nonetheless, gradient descent is useful for some\nproblems. Notably, (stochastic) gradient descent is used to fit neural\nnetworks, where the dimension of `x` is so large that computing the\ninverse hessian in (quasi) Newton's method is prohibitively time\nconsuming. \n\n# Newton's method\n\nNewton's method and its variations are often the most efficient\nminimization algorithms. Newton's method updates $x$ by minimizing a\nsecond order approximation to $f$. Specifically:\n\n1. Given $x$ set $x_{new} = x - (D^2f_x)^{-1} Df_x$\n2. Repeat until $\\norm{Df_{x}}$, $\\norm{x-x_{new}}$, and/or\n   $\\abs{f(x)-f(x_{new})}$ small."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "markdown && printfunc(\"newton\",\"smooth_optimizers.jl\");"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "result = newton(f, x0)\ngif(result[5], \"newton.gif\", fps=5)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "![newton](newton.gif)\n\n\nNewton's method tends to take relatively few iterations to converge\nfor well-behaved functions. It does have the disadvantage that hessian\nand its inverse can be time consuming to compute, especially when the\ndimension of $x$ is large. Newton's method can be unstable for\nfunctions that are not well approximated by their second\nexpansion. This problem can be mitigated by combining Newton's method\nwith a line search or trust region. \n\n## Line search\n\nLine searches consist of approximately minimizing $f$ along a given\ndirection instead of updating $x$ with a fixed step size. For Newton's\nmethod, instead of setting $x_{new} = x - (D^2f_x)^{-1} Df_x$, set \n$x_{new} \\approx \\argmin_{\\delta} f(x - \\delta (D^2f_x)^{-1} Df_x)$  where\n$\\delta$ is a scalar. This one dimensional problem can be solved\nfairly quickly. Line search can also be combined with gradient\ndescent. \n\n## Trust region\n\nInstead of setting \n$$\nx_{new} = x - (D^2f_x)^{-1} Df_x =\n\\argmin_{\\tilde{x}} f(x) + Df_x (\\tilde{x} - x) + \\frac{1}{2}\n(\\tilde{x}-x)^T Df_x (\\tilde{x} - x)\n$$\nto the unconstrained minimizer of a local second order approximation,\ntrust region methods introduce an region near $x$ where the\napproximation is trusted, and set\n$$\nx_{new} = \\argmin_{\\tilde{x} \\in TR(x)} f(x) + Df_x (\\tilde{x} - x) + \\frac{1}{2}\n(\\tilde{x}-x)^T D^2 f_x (\\tilde{x} - x).\n$$\nOften $TR(x) = \\{\\tilde{x} : \\norm{x - \\tilde{x}} < r\\}$. The radius\nof the trust region is then increased or decreased depending on\n$f(x_{new})$. \n\n## Quasi-Newton \n\nQuasi-Newton methods (in particular the BFGS algorithm) are probably\nthe most commonly used nonlinear optimization algorithm. Quasi-Newton\nmethods are similar to Newton's method, except instead of evaluating\nthe hessian directly, quasi-Newton methods build an approximation to\nthe hessian from repeated evaluations of $Df_x$ at different $x$.\n\nOptim.jl contains all the algorithms mentioned above. [Their advice on\nchoice of algorithm is worth\nfollowing.](https://julianlsolvers.github.io/Optim.jl/stable/#user/algochoice/). \n\n## Details matter in practice\n\nIn each of the algorithms above, we were somewhat cavalier with\ndetails like how to adjust step sizes and trust regions and what it\nmeans to approximately minimize during a line search. In practice\nthese details can be quite important for how long an algorithm takes\nand whether it succeeds or fails. Different implementations of\nalgorithms have different details. Often the details can be adjusted\nthrough some options. It can be worthwhile to try multiple\nimplementations and options to get the best performance. \n\n\n# Constrained optimization\n\nConstrained optimization is a bit harder than unconstrained, but uses\nsimilar ideas. For simple bound constraints, like $x\\geq 0$ it is\noften easiest to simply transform to an unconstrained case by\noptimizing over $y = \\log(x)$ instead. \n\nFor problems with equality constraints, one can apply Newton's method\nto the first order conditions. \n\nThe difficult case is when there are inequality constraints. Just like\nwhen solving analytically, the difficulty is figuring out which\nconstraints bind and which do not. \nFor inequality constraints, we will consider problems written in the form:\n$$\n\\min_{x \\in \\R^n} f(x) \\text{ s.t. } c(x) \\geq 0 \n$$\n\n## Interior Point Methods\n\nInterior point methods circumvent the problem of figuring out which\nconstraints bind by approaching the optimum from the interior of the\nfeasible set. To do this, the interior point method applies Newton's\nmethod to a modified version of the first order condition. The\nunmodified first order conditions can be written\n$$\n\\begin{align*}\n0 = & Df_x - \\lambda^T Dc_x \\\\\n0 = & \\lambda_i c_i(x) \\\\\n\\lambda \\geq & 0 \\\\\nc(x) \\geq & 0\n\\end{align*}\n$$\nA difficulty with these conditions is that solving them can require\nguessing and checking which combinations of constraints bind and which\ndo not. Interior point methods get around this problem by beginning\nwith an interior $x$ and $\\lambda$ such that $\\lambda>0$ and\n$c(x)>0$. They are then updated by applying Newton's method to the\nequations\n$$\n\\begin{align*}\n0 = & Df_x - \\lambda^T Dc_x \\\\\n\\mu = & \\lambda_i c_i(x) \\\\\n\\end{align*}\n$$\nwhere there is now a $\\mu$ in place of $0$ in the second equation. $x$\nand $\\lambda$ are updated according to Newton's method for this system\nof equations. In particular, \n$x_{new} = x + \\Delta_x$ and $\\lambda_{new}= \\lambda + \\Delta_\\lambda$, where\n$$\n\\begin{align*}\n\\begin{pmatrix} - ( Df_x - \\lambda^T Dc_x) \\\\\n\\mu 1_m -  diag(c(x)) \\lambda \n\\end{pmatrix} = \\begin{pmatrix} \n D^2 f_x -  D^2 (\\lambda c)_x  & -Dc_x^T \\\\\n \\lambda Dc_x & diag(c(x)) \n\\end{pmatrix} \\begin{pmatrix}\n\\Delta_x \\\\\n\\Delta_\\lambda\n\\end{pmatrix}\n\\end{align*}\n$$\nOver iterations $\\mu$ is gradually decreased toward\n$0$. Here is one simple implementation."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "markdown && printfunc(\"interiorpoint\",\"constrained_optimizers.jl\");"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "f = banana(1.0,1.0)\nx0 = [3.0, 0.0]\nfunction constraint(x)\n  [x[1] + x[2] - 2.5]\nend"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "result = interiorpoint(f, x0, constraint; maxiter=100)\ngif(result[5], \"ip.gif\", fps=5)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "![interior point](ip.gif)\n\n\nOptim.jl includes an interior point method. IPOPT is another popular\nimplementation. As above, the details of the algorithm can be\nimportant in practice. It can be worthwhile to experiment with\ndifferent methods for updating $\\mu$, using a more sophisticated line\nsearch or trust region, and perhaps replacing the computation of the\nhessian with a quasi-Newton approximation. \n\nIt has been proven that interior point methods converge relatively\nquickly for convex optimization problems. \n\n## Sequential quadratic programming\n\nSequential quadratic programming relies on the fact that there are\nefficient methods to compute the solution to quadratic programs ---\noptimization problems with quadratic objective functions and linear\nconstraints. We can then solve a more general optimization problem by\nsolving a sequence of quadratic programs that approximate the original problem.\n\nSequential quadratic programming is like a constrained version of\nNewton's method. Given a current $x$ and $\\lambda$ the new $x$ solves\n$$\n\\begin{align*}\nx_{new} \\in \\argmin_{\\tilde{x}} & f(x) + Df_x (\\tilde{x} - x) +\n\\frac{1}{2} (\\tilde{x}-x)^T (D^2 f_x + D^2 (\\lambda^T c)_x) (\\tilde{x} - x) \\\\\n \\text{ s. t. } & c(x) + Dc_{x} (\\tilde{x} - x) \\geq 0\n\\end{align*}\n$$\nand the new $\\lambda$ is set to the value of the multipliers for this\nproblem. \n\nThis quadratic program (an optimization problem with a quadratic\nobjective function and linear constraints) can be solved fairly\nefficiently if $(D^2 f_x + D^2 (\\lambda^T c)_x)$ is positive\nsemi-definite. \n\n!!! info Convex program solvers \n    Most for Convex program solvers are designed to accept semidefinite\n    programs instead of quadratic programs. A [quadratic program can be\n    re-written as a semidefinite\n    program](https://math.stackexchange.com/q/2256243). A solver such as\n    SCS, ECOS, or Mosek can then be used. Fortunately, `Convex.jl` will\n    automatically take care of any necessary transformation.\n    \nOne could also incorporate a trust region or line search into the\nabove algorithm. Here is one simple implementation."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "using Convex, ECOS"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "markdown && printfunc(\"sequentialquadratic\",\"constrained_optimizers.jl\");"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "x0 = [0.0, 0.0]\nresult = sequentialquadratic(f, x0, constraint; maxiter=100, verbosity=0)\ngif(result[5], \"sqp.gif\", fps=5)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "![sqp](sqp.gif)\n\nCompared to interior point methods, sequential quadratic programming\nhas the advantage of not needing a feasible point to begin, and often\ntaking fewer iteration. Like Newton's method, sequential quadratic\nprogramming has local quadratic convergence. A downside of sequential\nquadratic programming is that solving the quadratic program at each\nstep can take considerably longer than solving the system of linear\nequations that interior point methods and Newton methods require.\n\n\n## SLQP active Set \n\nSLQP active set methods use a linear approximation to the optimization\nproblem to decide which constraints are \"active\" (binding). In each\niteration, a linear approximation to the original problem is first\nsolved. The constraints that bind in linear approximation are then\nassumed to bind in the full problem, and we take a Newton step\naccordingly. \n\n\n## Augmented Lagrangian\n\nAugmented Lagragian methods convert a constrained minimization problem\nto an unconstrained problem by adding a penalty that increases with\nthe constraint violation to the Lagrangian. \n\n## Barrier methods\n\nBarrier methods refer to adding a penalty that increases toward\n$\\infty$ as the constraints get close to violated (such as\n$\\log(c(x))$). Barrier methods are closely related to interior point\nmethods. Applying Newton's method to a log-barrier penalized problem\ngives rise to something very similar to our `interiorpoint` algorithm\nabove. \n\n\n# Strategies for global optimization\n\nThe above algorithms will all converge to local minima. Finding a\nglobal minimum is generally very hard. There are a few algorithms that\nhave been proven to converge to a global optimum, such a DIRECT-L in\n`NLopt`. However, these algorithms are prohibitively time-consuming\nfor even moderate size problems. \n\nRandomization is a good strategy for avoiding local minima. Some\nalgorithms with randomization, like simulated annealing, can be shown\nto converge to the global optimum with high probability. In practice,\nthese are also often too inefficient for moderate size\nproblems. \n\nA good practical approach is to use an algorithm that combines\nrandomization with some model-based search. A common approach is to\nuse a variant of Newton's method starting from a bunch of\nrandomly chosen initial values. \n\nAlgorithms that combine a linear or quadratic approximation to the\nobjective function with some randomness in the search direction can\nalso be useful. An example is stochastic gradient descent, which is\noften used to fit neural networks. I have had good experience with\n[CMA-ES](http://cma.gforge.inria.fr/). It worked well to estimate the\nfinite mixture model in EFS (2015)[@efs2015]. \n\nBayesian methods can also be used for optimization and will naturally\ninclude some randomization in their search. Hamiltonian Monte-Carlo\nmethods, which incorporate gradient information in their search, are\nlikely to be efficient. See\n[`DynamicHMC.jl`](https://github.com/tpapp/DynamicHMC.jl).\n\n# References\n\n\\bibliography"
      ],
      "metadata": {}
    }
  ],
  "nbformat_minor": 2,
  "metadata": {
    "language_info": {
      "file_extension": ".jl",
      "mimetype": "application/julia",
      "name": "julia",
      "version": "1.2.0"
    },
    "kernelspec": {
      "name": "julia-1.2",
      "display_name": "Julia 1.2.0",
      "language": "julia"
    }
  },
  "nbformat": 4
}
