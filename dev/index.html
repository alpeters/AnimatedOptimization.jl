<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Home · AnimatedOptimization.jl</title><link href="https://cdnjs.cloudflare.com/ajax/libs/normalize/4.2.0/normalize.min.css" rel="stylesheet" type="text/css"/><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/default.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.2.0/require.min.js" data-main="assets/documenter.js"></script><script src="siteinfo.js"></script><script src="../versions.js"></script><link href="assets/documenter.css" rel="stylesheet" type="text/css"/></head><body><nav class="toc"><h1>AnimatedOptimization.jl</h1><select id="version-selector" onChange="window.location.href=this.value" style="visibility: hidden"></select><form class="search" id="search-form" action="search/"><input id="search-query" name="q" type="text" placeholder="Search docs"/></form><ul><li class="current"><a class="toctext" href>Home</a><ul class="internal"><li class="toplevel"><a class="toctext" href="#API-1">API</a></li></ul></li><li><a class="toctext" href="redirect/">Animations</a></li></ul></nav><article id="docs"><header><nav><ul><li><a href>Home</a></li></ul><a class="edit-page" href="https://github.com/schrimpf/AnimatedOptimization.jl/blob/master/docs/src/index.md#L"><span class="fa"></span> Edit on GitHub</a></nav><hr/><div id="topbar"><span>Home</span><a class="fa fa-bars" href="#"></a></div></header><h1><a class="nav-anchor" id="AnimatedOptimization.jl-1" href="#AnimatedOptimization.jl-1">AnimatedOptimization.jl</a></h1><p>Some optimization algorithms (for any function) with animations for functions from R² → R.</p><p>These are meant for teaching. They have not been extenisvely tested and are likely not well-suited for other uses. </p><p>For usage see <a href="optimization.html">this document</a> or <a href="optimization.ipynb">a notebook version</a>.</p><h1><a class="nav-anchor" id="API-1" href="#API-1">API</a></h1><ul><li><a href="#AnimatedOptimization.graddescent-Tuple{Any,Any}"><code>AnimatedOptimization.graddescent</code></a></li><li><a href="#AnimatedOptimization.interiorpoint-Tuple{Any,Any,Any}"><code>AnimatedOptimization.interiorpoint</code></a></li><li><a href="#AnimatedOptimization.minrandomsearch-Tuple{Any,Any,Any}"><code>AnimatedOptimization.minrandomsearch</code></a></li><li><a href="#AnimatedOptimization.newton-Tuple{Any,Any}"><code>AnimatedOptimization.newton</code></a></li><li><a href="#AnimatedOptimization.sequentialquadratic-Tuple{Any,Any,Any}"><code>AnimatedOptimization.sequentialquadratic</code></a></li></ul><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="AnimatedOptimization.graddescent-Tuple{Any,Any}" href="#AnimatedOptimization.graddescent-Tuple{Any,Any}"><code>AnimatedOptimization.graddescent</code></a> — <span class="docstring-category">Method</span>.</div><div><div><pre><code class="language-julia"> graddescent(f, x0; grad=x-&gt;Forwardiff.gradient(f,x),
             γ0=1.0, ftol = 1e-6,
             xtol = 1e-4, gtol=1-6, maxiter = 1000, 
             xrange=[-2., 3.],
             yrange=[-2.,6.], animate=true)</code></pre><p>Find the minimum of function <code>f</code> by gradient descent</p><p><strong>Arguments</strong></p><ul><li><code>f</code> function to minimize</li><li><code>x0</code> starting value</li><li><code>grad</code> function that computes gradient of <code>f</code></li><li><code>γ0</code> initial step size multiplier</li><li><code>ftol</code> tolerance for function value</li><li><code>xtol</code> tolerance for x</li><li><code>gtol</code> tolerance for gradient. Convergence requires meeting all three tolerances.</li><li><code>maxiter</code> maximum iterations</li><li><code>xrange</code> x-axis range for animation</li><li><code>yrange</code> y-axis range for animation</li><li><code>animate</code> whether to create animation</li></ul><p><strong>Returns</strong></p><ul><li><code>(fmin, xmin, iter, info, anim)</code> tuple consisting of minimal function value, minimizer, number of iterations, convergence info, and animations</li></ul></div></div><a class="source-link" target="_blank" href="https://github.com/schrimpf/AnimatedOptimization.jl/blob/d5512ecfac8d60964488fcca2c92fee9328ecce5/src/smooth_optimizers.jl#LL1-L28">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="AnimatedOptimization.interiorpoint-Tuple{Any,Any,Any}" href="#AnimatedOptimization.interiorpoint-Tuple{Any,Any,Any}"><code>AnimatedOptimization.interiorpoint</code></a> — <span class="docstring-category">Method</span>.</div><div><div><pre><code class="language-julia">interiorpoint(f, x0, c; 
              L   = (x,λ)-&gt;(f(x) - dot(λ,c(x))),
              ∇ₓL = (x,λ)-&gt;ForwardDiff.gradient(z-&gt;L(z,λ), x),
              ∇²ₓL= (x,λ)-&gt;ForwardDiff.hessian(z-&gt;L(z,λ), x),
              ∇c = x-&gt;ForwardDiff.jacobian(c,x),
              tol=1e-4, maxiter = 1000,
              μ0 = 1.0, μfactor = 0.2,
              xrange=[-2., 3.],
              yrange=[-2.,6.], animate=true)</code></pre><p>Find the minimum of function <code>f</code> subject to <code>c(x) &gt;= 0</code> using a primal-dual interior point method.</p><p><strong>Arguments</strong></p><ul><li><code>f</code> function to minimizie</li><li><code>x0</code> starting value. Must have c(x0) &gt; 0</li><li><code>c</code> constraint function. Must return an array.</li><li><code>L   = (x,λ)-&gt;(f(x) - dot(λ,c(x)))</code> Lagrangian</li><li><code>∇ₓL = (x,λ)-&gt;ForwardDiff.gradient(z-&gt;L(z,λ), x)</code> Derivative of Lagrangian wrt <code>x</code></li><li><code>∇²ₓL= (x,λ)-&gt;ForwardDiff.hessian(z-&gt;L(z,λ), x)</code> Hessian of Lagrangian wrt <code>x</code></li><li><code>∇c = x-&gt;ForwardDiff.jacobian(c,x)</code> Jacobian of constraints</li><li><code>tol</code> convergence tolerance</li><li><code>μ0</code> initial μ</li><li><code>μfactor</code> how much to decrease μ by</li><li><code>xrange</code> range of x-axis for animation</li><li><code>yrange</code> range of y-axis for animation</li><li><code>animate</code> whether to create an animation (if true requires length(x)==2)</li><li><code>verbosity</code> higher values result in more printed output during search. 0 for no output, any number &gt; 0 for some.  </li></ul><p><strong>Returns</strong></p><ul><li><code>(fmin, xmin, iter, info, animate)</code> tuple consisting of minimal function value, minimizer, number of iterations, and convergence info</li></ul></div></div><a class="source-link" target="_blank" href="https://github.com/schrimpf/AnimatedOptimization.jl/blob/d5512ecfac8d60964488fcca2c92fee9328ecce5/src/constrained_optimizers.jl#LL1-L37">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="AnimatedOptimization.minrandomsearch-Tuple{Any,Any,Any}" href="#AnimatedOptimization.minrandomsearch-Tuple{Any,Any,Any}"><code>AnimatedOptimization.minrandomsearch</code></a> — <span class="docstring-category">Method</span>.</div><div><div><pre><code class="language-julia">minrandomsearch(f, x0, npoints; var0=1.0, ftol = 1e-6,
                     vtol = 1e-4, maxiter = 1000,
                     vshrink=0.9, xrange=[-2., 3.],
                     yrange=[-2.,6.])</code></pre><p>Find the minimum of function <code>f</code> by random search. </p><p>Creates an animation illustrating search progress.</p><p><strong>Arguments</strong></p><ul><li><code>f</code> function to minimizie</li><li><code>x0</code> starting value</li><li><code>npoints</code> number of points in cloud</li><li><code>var0</code> initial variance of points</li><li><code>ftol</code> convergence tolerance for function value. Search terminates if both function change is less than ftol and variance is less than vtol.</li><li><code>vtol</code> convergence tolerance for variance. Search terminates if both function change is less than ftol and variance is less than vtol.</li><li><code>maxiter</code> maximum number of iterations</li><li><code>vshrink</code> after every 100 iterations with no function improvement, the variance is reduced by this factor</li><li><code>xrange</code> range of x-axis in animation</li><li><code>yrange</code> range of y-axis in animation</li><li><code>animate</code> whether to create animation</li></ul><p><strong>Returns</strong></p><ul><li><code>(fmin, xmin, iter, info, anim)</code> tuple consisting of minimal function value, minimizer, number of iterations, convergence info, and an animation</li></ul></div></div><a class="source-link" target="_blank" href="https://github.com/schrimpf/AnimatedOptimization.jl/blob/d5512ecfac8d60964488fcca2c92fee9328ecce5/src/heuristic_optimizers.jl#LL1-L29">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="AnimatedOptimization.newton-Tuple{Any,Any}" href="#AnimatedOptimization.newton-Tuple{Any,Any}"><code>AnimatedOptimization.newton</code></a> — <span class="docstring-category">Method</span>.</div><div><div><pre><code class="language-julia">newton(f, x0; 
       grad=x-&gt;ForwardDiff.gradient(f,x),
       hess=x-&gt;ForwardDiff.hessian(f,x),
       ftol = 1e-6,
       xtol = 1e-4, gtol=1-6, maxiter = 1000, 
       xrange=[-2., 3.],
       yrange=[-2.,6.], animate=true)</code></pre><p>Find the minimum of function <code>f</code> by Newton&#39;s method.</p><p><strong>Arguments</strong></p><ul><li><code>f</code> function to minimizie</li><li><code>x0</code> starting value</li><li><code>grad</code> function that returns gradient of <code>f</code></li><li><code>hess</code> function that returns hessian of <code>f</code></li><li><code>ftol</code> tolerance for function value</li><li><code>xtol</code> tolerance for x</li><li><code>gtol</code> tolerance for gradient. Convergence requires meeting all three tolerances.</li><li><code>maxiter</code> maximum iterations</li><li><code>xrange</code> x-axis range for animation</li><li><code>yrange</code> y-axis range for animation</li><li><code>animate</code> whether to create animation</li></ul><p><strong>Returns</strong></p><ul><li><code>(fmin, xmin, iter, info, anim)</code> tuple consisting of minimal function value, minimizer, number of iterations, convergence info, and animation</li></ul></div></div><a class="source-link" target="_blank" href="https://github.com/schrimpf/AnimatedOptimization.jl/blob/d5512ecfac8d60964488fcca2c92fee9328ecce5/src/smooth_optimizers.jl#LL102-L132">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="AnimatedOptimization.sequentialquadratic-Tuple{Any,Any,Any}" href="#AnimatedOptimization.sequentialquadratic-Tuple{Any,Any,Any}"><code>AnimatedOptimization.sequentialquadratic</code></a> — <span class="docstring-category">Method</span>.</div><div><div><pre><code class="language-julia">  sequentialquadratic(f, x0, c; 
                      ∇f = x-&gt;ForwardDiff.gradient(f,x),
                      ∇c = x-&gt;ForwardDiff.jacobian(c,x),
                      L   = (x,λ)-&gt;(f(x) - dot(λ,c(x))),
                      ∇ₓL = (x,λ)-&gt;ForwardDiff.gradient(z-&gt;L(z,λ), x),
                      ∇²ₓL= (x,λ)-&gt;ForwardDiff.hessian(z-&gt;L(z,λ), x),
                      tol=1e-4, maxiter = 1000,
                      trustradius=1.0, xrange=[-2., 3.],
                      yrange=[-2.,6.], animate=true, verbosity=1)</code></pre><p>Find the minimum of function <code>f</code> by random search</p><p><strong>Arguments</strong></p><ul><li><code>f</code> function to minimizie</li><li><code>x0</code> starting value. Must have c(x0) &gt; 0</li><li><code>c</code> constraint function. Must return an array.</li><li><code>∇f = x-&gt;ForwardDiff.gradient(f,x)</code></li><li><code>∇c = x-&gt;ForwardDiff.jacobian(c,x)</code> Jacobian of constraints</li><li><code>L   = (x,λ)-&gt;(f(x) - dot(λ,c(x)))</code> Lagrangian</li><li><code>∇ₓL = (x,λ)-&gt;ForwardDiff.gradient(z-&gt;L(z,λ), x)</code> Derivative of Lagrangian wrt <code>x</code></li><li><code>∇²ₓL= (x,λ)-&gt;ForwardDiff.hessian(z-&gt;L(z,λ), x)</code> Hessian of Lagrangian wrt <code>x</code></li><li><code>tol</code> convergence tolerance</li><li><code>maxiter</code></li><li><code>trustradius</code> initial trust region radius</li><li><code>xrange</code> range of x-axis for animation</li><li><code>yrange</code> range of y-axis for animation</li><li><code>animate</code> whether to create an animation (if true requires length(x)==2)</li><li><code>verbosity</code> higher values result in more printed output during search. 0 for no output, any number &gt; 0 for some.  </li></ul><p><strong>Returns</strong></p><ul><li><code>(fmin, xmin, iter, info, animate)</code> tuple consisting of minimal function value, minimizer, number of iterations, and convergence info</li></ul></div></div><a class="source-link" target="_blank" href="https://github.com/schrimpf/AnimatedOptimization.jl/blob/d5512ecfac8d60964488fcca2c92fee9328ecce5/src/constrained_optimizers.jl#LL137-L174">source</a></section><footer><hr/><a class="next" href="redirect/"><span class="direction">Next</span><span class="title">Animations</span></a></footer></article></body></html>
